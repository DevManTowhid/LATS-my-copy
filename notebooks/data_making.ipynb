{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65bac36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PegasusTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load tokenizer and dataset\n",
    "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-cnn_dailymail')\n",
    "dataset = load_dataset(\"cnn_dailymail\", split=\"train[:10%]\")\n",
    "\n",
    "# Tokenize the data\n",
    "def tokenize_data(examples):\n",
    "    inputs = tokenizer(examples['article'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    targets = tokenizer(examples['highlights'], padding=\"max_length\", truncation=True, max_length=150)\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "# Save the tokenized dataset\n",
    "tokenized_dataset.save_to_disk('data/processed')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
